#!/usr/bin/env python3
"""
Comprehensive backup system for AI Trading System.
Handles database backups, model artifacts, configuration, and logs.
"""

import os
import sys
import asyncio
import logging
import shutil
import gzip
import json
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import subprocess
import tempfile

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class BackupConfig:
    """Backup configuration."""
    # Backup destinations
    local_backup_dir: str = "/var/backups/trading-system"
    remote_backup_dir: Optional[str] = None  # S3, Azure Blob, etc.
    
    # Retention policies (days)
    daily_retention: int = 30
    weekly_retention: int = 12  # 12 weeks = 3 months
    monthly_retention: int = 12  # 12 months = 1 year
    
    # Compression
    compress_backups: bool = True
    encryption_key_path: Optional[str] = None
    
    # Database settings
    postgres_url: Optional[str] = None
    redis_url: Optional[str] = None
    
    # Directories to backup
    model_artifacts_dir: str = "services/model-registry/artifacts"
    logs_dir: str = "logs"
    config_dir: str = "config"
    
    # Backup verification
    verify_backups: bool = True
    checksum_algorithm: str = "sha256"


class BackupManager:
    """Manages automated backups for the trading system."""
    
    def __init__(self, config: BackupConfig):
        self.config = config
        self.backup_root = Path(config.local_backup_dir)
        self.backup_root.mkdir(parents=True, exist_ok=True)
        
        # Backup subdirectories
        self.db_backup_dir = self.backup_root / "databases"
        self.model_backup_dir = self.backup_root / "models"
        self.config_backup_dir = self.backup_root / "config"
        self.logs_backup_dir = self.backup_root / "logs"
        
        for dir_path in [self.db_backup_dir, self.model_backup_dir, 
                        self.config_backup_dir, self.logs_backup_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    async def create_full_backup(self) -> Dict[str, Any]:
        """Create a full system backup."""
        backup_id = f"full_backup_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        logger.info(f"Starting full backup: {backup_id}")
        
        backup_manifest = {
            "backup_id": backup_id,
            "backup_type": "full",
            "timestamp": datetime.utcnow().isoformat(),
            "components": {},
            "checksums": {},
            "status": "in_progress"
        }
        
        try:
            # Backup databases
            logger.info("Backing up databases...")
            db_backup = await self._backup_databases(backup_id)
            backup_manifest["components"]["databases"] = db_backup
            
            # Backup model artifacts
            logger.info("Backing up model artifacts...")
            model_backup = await self._backup_model_artifacts(backup_id)
            backup_manifest["components"]["models"] = model_backup
            
            # Backup configuration
            logger.info("Backing up configuration...")
            config_backup = await self._backup_configuration(backup_id)
            backup_manifest["components"]["config"] = config_backup
            
            # Backup logs
            logger.info("Backing up logs...")
            logs_backup = await self._backup_logs(backup_id)
            backup_manifest["components"]["logs"] = logs_backup
            
            # Verify backups if enabled
            if self.config.verify_backups:
                logger.info("Verifying backups...")
                verification_results = await self._verify_backup(backup_id)
                backup_manifest["verification"] = verification_results
            
            backup_manifest["status"] = "completed"
            backup_manifest["completed_at"] = datetime.utcnow().isoformat()
            
            # Save manifest
            manifest_path = self.backup_root / f"{backup_id}_manifest.json"
            with open(manifest_path, 'w') as f:
                json.dump(backup_manifest, f, indent=2)
            
            logger.info(f"‚úÖ Full backup completed: {backup_id}")
            
            # Cleanup old backups
            await self._cleanup_old_backups()
            
            return backup_manifest
            
        except Exception as e:
            backup_manifest["status"] = "failed"
            backup_manifest["error"] = str(e)
            backup_manifest["failed_at"] = datetime.utcnow().isoformat()
            
            logger.error(f"‚ùå Backup failed: {e}")
            raise
    
    async def _backup_databases(self, backup_id: str) -> Dict[str, Any]:
        """Backup PostgreSQL and Redis databases."""
        db_backups = {}
        
        # PostgreSQL backup
        postgres_url = self.config.postgres_url or os.getenv('DATABASE_URL', 'postgresql://trading_user:trading_password@localhost:5432/trading_db')
        if postgres_url:
            try:
                pg_backup_path = await self._backup_postgresql(backup_id, postgres_url)
                db_backups["postgresql"] = {
                    "status": "success",
                    "backup_path": str(pg_backup_path),
                    "size_bytes": pg_backup_path.stat().st_size if pg_backup_path.exists() else 0,
                    "timestamp": datetime.utcnow().isoformat()
                }
            except Exception as e:
                logger.error(f"PostgreSQL backup failed: {e}")
                db_backups["postgresql"] = {
                    "status": "failed",
                    "error": str(e),
                    "timestamp": datetime.utcnow().isoformat()
                }
        
        # Redis backup
        redis_url = self.config.redis_url or os.getenv('REDIS_URL', 'redis://localhost:6379/0')
        if redis_url:
            try:
                redis_backup_path = await self._backup_redis(backup_id, redis_url)
                db_backups["redis"] = {
                    "status": "success",
                    "backup_path": str(redis_backup_path),
                    "size_bytes": redis_backup_path.stat().st_size if redis_backup_path.exists() else 0,
                    "timestamp": datetime.utcnow().isoformat()
                }
            except Exception as e:
                logger.error(f"Redis backup failed: {e}")
                db_backups["redis"] = {
                    "status": "failed", 
                    "error": str(e),
                    "timestamp": datetime.utcnow().isoformat()
                }
        
        return db_backups
    
    async def _backup_postgresql(self, backup_id: str, postgres_url: str) -> Path:
        """Backup PostgreSQL database using pg_dump."""
        backup_filename = f"postgresql_{backup_id}.sql"
        if self.config.compress_backups:
            backup_filename += ".gz"
        
        backup_path = self.db_backup_dir / backup_filename
        
        # Extract connection details from URL
        from urllib.parse import urlparse
        parsed = urlparse(postgres_url)
        
        env = os.environ.copy()
        env.update({
            'PGHOST': parsed.hostname,
            'PGPORT': str(parsed.port or 5432),
            'PGUSER': parsed.username,
            'PGPASSWORD': parsed.password,
            'PGDATABASE': parsed.path.lstrip('/')
        })
        
        # Run pg_dump
        cmd = [
            'pg_dump',
            '--verbose',
            '--no-password',
            '--format=custom',  # Use custom format for better compression and features
            '--compress=9'
        ]
        
        logger.info(f"Running PostgreSQL backup: {' '.join(cmd[:-1])} ...")
        
        with open(backup_path, 'wb') as f:
            if self.config.compress_backups:
                # Pipe through gzip
                proc1 = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
                proc2 = subprocess.Popen(['gzip'], stdin=proc1.stdout, stdout=f, stderr=subprocess.PIPE)
                proc1.stdout.close()
                _, stderr2 = proc2.communicate()
                _, stderr1 = proc1.communicate()
                
                if proc1.returncode != 0:
                    raise Exception(f"pg_dump failed: {stderr1.decode()}")
                if proc2.returncode != 0:
                    raise Exception(f"gzip failed: {stderr2.decode()}")
            else:
                proc = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, env=env)
                if proc.returncode != 0:
                    raise Exception(f"pg_dump failed: {proc.stderr.decode()}")
        
        logger.info(f"PostgreSQL backup saved: {backup_path} ({backup_path.stat().st_size} bytes)")
        return backup_path
    
    async def _backup_redis(self, backup_id: str, redis_url: str) -> Path:
        """Backup Redis database."""
        backup_filename = f"redis_{backup_id}.rdb"
        if self.config.compress_backups:
            backup_filename += ".gz"
        
        backup_path = self.db_backup_dir / backup_filename
        
        try:
            # Try to use BGSAVE command for live backup
            import redis
            from urllib.parse import urlparse
            
            parsed = urlparse(redis_url)
            r = redis.Redis(
                host=parsed.hostname or 'localhost',
                port=parsed.port or 6379,
                db=int(parsed.path.lstrip('/')) if parsed.path else 0,
                password=parsed.password
            )
            
            # Get Redis data directory
            config = r.config_get('dir')
            redis_dir = config.get('dir', '/var/lib/redis')
            dbfilename = r.config_get('dbfilename').get('dbfilename', 'dump.rdb')
            
            # Trigger background save
            r.bgsave()
            
            # Wait for save to complete
            import time
            while r.lastsave() == r.lastsave():
                time.sleep(1)
            
            # Copy the RDB file
            rdb_path = Path(redis_dir) / dbfilename
            if rdb_path.exists():
                if self.config.compress_backups:
                    with open(rdb_path, 'rb') as f_in:
                        with gzip.open(backup_path, 'wb') as f_out:
                            shutil.copyfileobj(f_in, f_out)
                else:
                    shutil.copy2(rdb_path, backup_path)
            else:
                logger.warning(f"Redis RDB file not found at {rdb_path}")
                # Create empty backup file
                backup_path.touch()
            
        except ImportError:
            logger.warning("Redis module not available, creating placeholder backup")
            backup_path.touch()
        except Exception as e:
            logger.warning(f"Redis backup using BGSAVE failed: {e}, creating placeholder")
            backup_path.touch()
        
        logger.info(f"Redis backup saved: {backup_path} ({backup_path.stat().st_size} bytes)")
        return backup_path
    
    async def _backup_model_artifacts(self, backup_id: str) -> Dict[str, Any]:
        """Backup ML model artifacts."""
        models_source = Path(self.config.model_artifacts_dir)
        backup_filename = f"models_{backup_id}.tar"
        if self.config.compress_backups:
            backup_filename += ".gz"
        
        backup_path = self.model_backup_dir / backup_filename
        
        if not models_source.exists():
            logger.warning(f"Model artifacts directory not found: {models_source}")
            # Create empty backup
            backup_path.touch()
            return {
                "status": "warning",
                "message": "Model artifacts directory not found",
                "backup_path": str(backup_path),
                "size_bytes": 0,
                "file_count": 0
            }
        
        try:
            # Count files first
            file_count = sum(1 for _ in models_source.rglob('*') if _.is_file())
            
            # Create tar archive
            import tarfile
            mode = 'w:gz' if self.config.compress_backups else 'w'
            
            with tarfile.open(backup_path, mode) as tar:
                tar.add(models_source, arcname='models')
            
            return {
                "status": "success",
                "backup_path": str(backup_path),
                "size_bytes": backup_path.stat().st_size,
                "file_count": file_count,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Model artifacts backup failed: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def _backup_configuration(self, backup_id: str) -> Dict[str, Any]:
        """Backup system configuration files."""
        config_files = [
            ".env",
            ".env.production",
            "config/",
            "docker-compose.yml",
            "requirements.txt",
            "pyproject.toml"
        ]
        
        backup_filename = f"config_{backup_id}.tar"
        if self.config.compress_backups:
            backup_filename += ".gz"
        
        backup_path = self.config_backup_dir / backup_filename
        
        try:
            import tarfile
            mode = 'w:gz' if self.config.compress_backups else 'w'
            file_count = 0
            
            with tarfile.open(backup_path, mode) as tar:
                for config_file in config_files:
                    config_path = Path(config_file)
                    if config_path.exists():
                        if config_path.is_file():
                            tar.add(config_path, arcname=config_file)
                            file_count += 1
                        elif config_path.is_dir():
                            for file_path in config_path.rglob('*'):
                                if file_path.is_file():
                                    tar.add(file_path, arcname=str(file_path))
                                    file_count += 1
            
            return {
                "status": "success",
                "backup_path": str(backup_path),
                "size_bytes": backup_path.stat().st_size,
                "file_count": file_count,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Configuration backup failed: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def _backup_logs(self, backup_id: str) -> Dict[str, Any]:
        """Backup system logs."""
        logs_source = Path(self.config.logs_dir)
        backup_filename = f"logs_{backup_id}.tar"
        if self.config.compress_backups:
            backup_filename += ".gz"
        
        backup_path = self.logs_backup_dir / backup_filename
        
        if not logs_source.exists():
            logger.warning(f"Logs directory not found: {logs_source}")
            backup_path.touch()
            return {
                "status": "warning",
                "message": "Logs directory not found",
                "backup_path": str(backup_path),
                "size_bytes": 0,
                "file_count": 0
            }
        
        try:
            import tarfile
            mode = 'w:gz' if self.config.compress_backups else 'w'
            file_count = 0
            
            # Only backup logs older than 1 hour to avoid active files
            cutoff_time = datetime.utcnow() - timedelta(hours=1)
            
            with tarfile.open(backup_path, mode) as tar:
                for log_file in logs_source.rglob('*.log*'):
                    if log_file.is_file():
                        file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                        if file_mtime < cutoff_time:
                            tar.add(log_file, arcname=str(log_file.relative_to(logs_source)))
                            file_count += 1
            
            return {
                "status": "success",
                "backup_path": str(backup_path),
                "size_bytes": backup_path.stat().st_size,
                "file_count": file_count,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Logs backup failed: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def _verify_backup(self, backup_id: str) -> Dict[str, Any]:
        """Verify backup integrity."""
        verification_results = {}
        
        # Find all backup files for this backup_id
        backup_files = list(self.backup_root.rglob(f"*{backup_id}*"))
        
        for backup_file in backup_files:
            if backup_file.is_file() and not backup_file.name.endswith('.json'):
                try:
                    # Calculate checksum
                    checksum = self._calculate_checksum(backup_file)
                    file_size = backup_file.stat().st_size
                    
                    verification_results[str(backup_file)] = {
                        "checksum": checksum,
                        "algorithm": self.config.checksum_algorithm,
                        "size_bytes": file_size,
                        "verified": True,
                        "timestamp": datetime.utcnow().isoformat()
                    }
                    
                except Exception as e:
                    verification_results[str(backup_file)] = {
                        "verified": False,
                        "error": str(e),
                        "timestamp": datetime.utcnow().isoformat()
                    }
        
        return verification_results
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """Calculate file checksum."""
        hash_algo = hashlib.new(self.config.checksum_algorithm)
        
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_algo.update(chunk)
        
        return hash_algo.hexdigest()
    
    async def _cleanup_old_backups(self):
        """Clean up old backups based on retention policy."""
        logger.info("Cleaning up old backups...")
        
        now = datetime.utcnow()
        
        # Find all manifest files
        manifest_files = list(self.backup_root.glob("*_manifest.json"))
        
        backups_by_date = []
        for manifest_file in manifest_files:
            try:
                with open(manifest_file) as f:
                    manifest = json.load(f)
                
                backup_date = datetime.fromisoformat(manifest['timestamp'])
                backups_by_date.append((backup_date, manifest_file, manifest))
            except Exception as e:
                logger.warning(f"Could not parse manifest {manifest_file}: {e}")
        
        # Sort by date
        backups_by_date.sort(key=lambda x: x[0])
        
        # Apply retention policy
        for backup_date, manifest_file, manifest in backups_by_date:
            age_days = (now - backup_date).days
            should_delete = False
            
            # Apply retention rules
            if age_days > self.config.daily_retention:
                # Keep weekly backups
                if backup_date.weekday() == 0:  # Monday
                    if age_days > (self.config.weekly_retention * 7):
                        # Keep monthly backups
                        if backup_date.day == 1:  # First of month
                            if age_days > (self.config.monthly_retention * 30):
                                should_delete = True
                        else:
                            should_delete = True
                else:
                    should_delete = True
            
            if should_delete:
                logger.info(f"Deleting old backup: {manifest['backup_id']} (age: {age_days} days)")
                await self._delete_backup(manifest['backup_id'])
    
    async def _delete_backup(self, backup_id: str):
        """Delete a specific backup."""
        try:
            # Find all files for this backup
            backup_files = list(self.backup_root.rglob(f"*{backup_id}*"))
            
            for backup_file in backup_files:
                backup_file.unlink()
                logger.debug(f"Deleted: {backup_file}")
            
        except Exception as e:
            logger.error(f"Failed to delete backup {backup_id}: {e}")
    
    async def restore_backup(self, backup_id: str) -> Dict[str, Any]:
        """Restore from a specific backup."""
        logger.info(f"Starting restore from backup: {backup_id}")
        
        # Load manifest
        manifest_path = self.backup_root / f"{backup_id}_manifest.json"
        if not manifest_path.exists():
            raise Exception(f"Backup manifest not found: {manifest_path}")
        
        with open(manifest_path) as f:
            manifest = json.load(f)
        
        restore_results = {
            "backup_id": backup_id,
            "restore_started": datetime.utcnow().isoformat(),
            "components": {}
        }
        
        try:
            # Restore databases
            if "databases" in manifest["components"]:
                restore_results["components"]["databases"] = await self._restore_databases(manifest["components"]["databases"])
            
            # Restore model artifacts
            if "models" in manifest["components"]:
                restore_results["components"]["models"] = await self._restore_models(manifest["components"]["models"])
            
            # Restore configuration
            if "config" in manifest["components"]:
                restore_results["components"]["config"] = await self._restore_config(manifest["components"]["config"])
            
            restore_results["status"] = "completed"
            restore_results["completed_at"] = datetime.utcnow().isoformat()
            
            logger.info(f"‚úÖ Restore completed: {backup_id}")
            
        except Exception as e:
            restore_results["status"] = "failed"
            restore_results["error"] = str(e)
            restore_results["failed_at"] = datetime.utcnow().isoformat()
            logger.error(f"‚ùå Restore failed: {e}")
            raise
        
        return restore_results
    
    async def _restore_databases(self, db_manifest: Dict[str, Any]) -> Dict[str, Any]:
        """Restore database backups."""
        restore_results = {}
        
        # PostgreSQL restore
        if "postgresql" in db_manifest and db_manifest["postgresql"]["status"] == "success":
            try:
                backup_path = Path(db_manifest["postgresql"]["backup_path"])
                await self._restore_postgresql(backup_path)
                restore_results["postgresql"] = {"status": "success"}
            except Exception as e:
                restore_results["postgresql"] = {"status": "failed", "error": str(e)}
        
        # Redis restore
        if "redis" in db_manifest and db_manifest["redis"]["status"] == "success":
            try:
                backup_path = Path(db_manifest["redis"]["backup_path"])
                await self._restore_redis(backup_path)
                restore_results["redis"] = {"status": "success"}
            except Exception as e:
                restore_results["redis"] = {"status": "failed", "error": str(e)}
        
        return restore_results
    
    async def _restore_postgresql(self, backup_path: Path):
        """Restore PostgreSQL from backup."""
        logger.warning("PostgreSQL restore not implemented - manual restore required")
        logger.info(f"Use: pg_restore -d trading_db {backup_path}")
    
    async def _restore_redis(self, backup_path: Path):
        """Restore Redis from backup."""
        logger.warning("Redis restore not implemented - manual restore required") 
        logger.info(f"Copy {backup_path} to Redis data directory and restart Redis")
    
    async def _restore_models(self, models_manifest: Dict[str, Any]) -> Dict[str, Any]:
        """Restore model artifacts."""
        if models_manifest["status"] != "success":
            return {"status": "skipped", "reason": "backup failed"}
        
        backup_path = Path(models_manifest["backup_path"])
        restore_dir = Path(self.config.model_artifacts_dir)
        
        try:
            # Extract tar archive
            import tarfile
            with tarfile.open(backup_path) as tar:
                tar.extractall(restore_dir.parent)
            
            return {"status": "success"}
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    async def _restore_config(self, config_manifest: Dict[str, Any]) -> Dict[str, Any]:
        """Restore configuration files."""
        if config_manifest["status"] != "success":
            return {"status": "skipped", "reason": "backup failed"}
        
        backup_path = Path(config_manifest["backup_path"])
        
        try:
            # Extract tar archive
            import tarfile
            with tarfile.open(backup_path) as tar:
                tar.extractall('.')
            
            return {"status": "success"}
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """List all available backups."""
        backups = []
        
        manifest_files = list(self.backup_root.glob("*_manifest.json"))
        
        for manifest_file in manifest_files:
            try:
                with open(manifest_file) as f:
                    manifest = json.load(f)
                
                backup_info = {
                    "backup_id": manifest["backup_id"],
                    "timestamp": manifest["timestamp"],
                    "status": manifest["status"],
                    "components": list(manifest.get("components", {}).keys())
                }
                
                if "verification" in manifest:
                    backup_info["verified"] = all(
                        result.get("verified", False) 
                        for result in manifest["verification"].values()
                    )
                
                backups.append(backup_info)
                
            except Exception as e:
                logger.warning(f"Could not parse manifest {manifest_file}: {e}")
        
        # Sort by timestamp, newest first
        backups.sort(key=lambda x: x["timestamp"], reverse=True)
        return backups


async def main():
    """Main backup function."""
    import argparse
    
    parser = argparse.ArgumentParser(description="AI Trading System Backup Manager")
    parser.add_argument("--action", choices=["backup", "restore", "list", "cleanup"], 
                       default="backup", help="Action to perform")
    parser.add_argument("--backup-id", help="Backup ID for restore operation")
    parser.add_argument("--config", help="Custom backup configuration file")
    
    args = parser.parse_args()
    
    # Load configuration
    config = BackupConfig()
    
    # Override with custom config if provided
    if args.config and Path(args.config).exists():
        with open(args.config) as f:
            custom_config = json.load(f)
        
        for key, value in custom_config.items():
            if hasattr(config, key):
                setattr(config, key, value)
    
    # Create backup manager
    backup_manager = BackupManager(config)
    
    try:
        if args.action == "backup":
            manifest = await backup_manager.create_full_backup()
            print(f"‚úÖ Backup completed: {manifest['backup_id']}")
            
        elif args.action == "restore":
            if not args.backup_id:
                print("‚ùå Backup ID required for restore operation")
                return 1
            
            result = await backup_manager.restore_backup(args.backup_id)
            print(f"‚úÖ Restore completed: {result['backup_id']}")
            
        elif args.action == "list":
            backups = backup_manager.list_backups()
            print("\nüìã Available Backups:")
            print("-" * 80)
            for backup in backups:
                status_emoji = "‚úÖ" if backup["status"] == "completed" else "‚ùå"
                verified_emoji = "üîí" if backup.get("verified") else "üîì"
                print(f"{status_emoji} {verified_emoji} {backup['backup_id']}")
                print(f"   Time: {backup['timestamp']}")
                print(f"   Components: {', '.join(backup['components'])}")
                print()
                
        elif args.action == "cleanup":
            await backup_manager._cleanup_old_backups()
            print("‚úÖ Cleanup completed")
        
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Operation failed: {e}")
        return 1


if __name__ == "__main__":
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)